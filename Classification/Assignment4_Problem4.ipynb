{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#To see all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.metrics import mean_squared_error   #RMSE\n",
    "\n",
    "\n",
    "import itertools \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4\n",
    "ebay_data = pd.read_csv(\"/Users/nivethida/Documents/Spring Sem/DataMining/Classification/eBayAuctions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1972 entries, 0 to 1971\n",
      "Data columns (total 8 columns):\n",
      "Category        1972 non-null object\n",
      "currency        1972 non-null object\n",
      "sellerRating    1972 non-null int64\n",
      "Duration        1972 non-null int64\n",
      "endDay          1972 non-null object\n",
      "ClosePrice      1972 non-null float64\n",
      "OpenPrice       1972 non-null float64\n",
      "Competitive?    1972 non-null int64\n",
      "dtypes: float64(2), int64(3), object(3)\n",
      "memory usage: 123.4+ KB\n"
     ]
    }
   ],
   "source": [
    "ebay_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ebay_data.drop(columns=['Competitive?'])\n",
    "x = pd.get_dummies(x)\n",
    "y = ebay_data['Competitive?']\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(x, y, test_size=0.40, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tp(actual,propensity,cutoff):\n",
    "    \n",
    "    cm = confusion_matrix(actual,[1 if p > cutoff else 0 for p in propensity]).ravel()\n",
    "    \n",
    "    tn=cm[0]\n",
    "    fp=cm[1]\n",
    "    fn=cm[2]\n",
    "    tp=cm[3]\n",
    "    true_positive_rate = tp / (tp + fn)\n",
    "    sensitivity = true_positive_rate\n",
    "    recall  = true_positive_rate\n",
    "    hit_rate = true_positive_rate\n",
    "    precision= tp / (tp + fp)\n",
    "    positive_predictive_value=precision\n",
    "\n",
    "    roc_auc_scor=roc_auc_score(actual,[1 if p > cutoff else 0 for p in propensity])\n",
    "    \n",
    "    return \"true negative \"+ str(tn), \\\n",
    "           \"false positive \"+str(fp),\\\n",
    "           \"false negative \"+str(fn),\\\n",
    "           \"true positive \"+str(tp),\\\n",
    "           \"true positive rate \"+str(true_positive_rate),\\\n",
    "           \"sensitivity \"+str(sensitivity),\\\n",
    "           \"recall \"+str(recall),\\\n",
    "           \"hit rate \"+str(hit_rate),\\\n",
    "           \"precision \"+str(precision),\\\n",
    "           \"positive predictive value \" + str(positive_predictive_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=1, splitter='best')\n",
      "('true negative 312', 'false positive 43', 'false negative 64', 'true positive 370', 'true positive rate 0.8525345622119815', 'sensitivity 0.8525345622119815', 'recall 0.8525345622119815', 'hit rate 0.8525345622119815', 'precision 0.8958837772397095', 'positive predictive value 0.8958837772397095')\n",
      "--------------------------------------------------\n",
      "accuracy\n",
      "0.8643852978453739\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "param_grid = {}\n",
    "\n",
    "gridSearch = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1)\n",
    "gridSearch.fit(train_X, train_y)\n",
    "\n",
    "clf = gridSearch.best_estimator_\n",
    "\n",
    "print(clf)\n",
    "                  \n",
    "print(eval_tp(actual=valid_y,\n",
    "        propensity=pd.DataFrame(clf.predict_proba(valid_X)).iloc[:,1],\n",
    "        cutoff=0.5))\n",
    "\n",
    "tree_reg_proba = clf.predict_proba(valid_X)\n",
    "\n",
    "df_valid=pd.DataFrame()\n",
    "df_valid['class']=valid_y\n",
    "df_valid['prob_0']=tree_reg_proba[:,0]\n",
    "df_valid['prob_1']=tree_reg_proba[:,1]\n",
    "df_valid['y_pred']=[1 if p > 0.5 else 0 for p in df_valid.prob_1]\n",
    "df_valid\n",
    "print('-'*50)\n",
    "print(\"accuracy\")\n",
    "print(accuracy_score(df_valid['class'],df_valid['y_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoostClassifier with DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
      "                                                         class_weight=None,\n",
      "                                                         criterion='gini',\n",
      "                                                         max_depth=6,\n",
      "                                                         max_features=10,\n",
      "                                                         max_leaf_nodes=None,\n",
      "                                                         min_impurity_decrease=0.0,\n",
      "                                                         min_impurity_split=None,\n",
      "                                                         min_samples_leaf=5,\n",
      "                                                         min_samples_split=2,\n",
      "                                                         min_weight_fraction_leaf=0.0,\n",
      "                                                         presort='deprecated',\n",
      "                                                         random_state=1,\n",
      "                                                         splitter='best'),\n",
      "                   learning_rate=1.0, n_estimators=100, random_state=1)\n",
      "('true negative 313', 'false positive 42', 'false negative 64', 'true positive 370', 'true positive rate 0.8525345622119815', 'sensitivity 0.8525345622119815', 'recall 0.8525345622119815', 'hit rate 0.8525345622119815', 'precision 0.8980582524271845', 'positive predictive value 0.8980582524271845')\n",
      "--------------------------------------------------\n",
      "accuracy\n",
      "0.8656527249683144\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=6,min_samples_split=2,\n",
    "                                               min_samples_leaf=5,max_features=10,random_state=1), \n",
    "                                               n_estimators=100, random_state=1)\n",
    "\n",
    "param_grid = {}\n",
    "gridSearch = GridSearchCV(clf, \n",
    "                          param_grid, cv=5, n_jobs=-1)\n",
    "gridSearch.fit(train_X, train_y)\n",
    "\n",
    "clf = gridSearch.best_estimator_\n",
    "\n",
    "print(clf)\n",
    "                        \n",
    "print(eval_tp(actual=valid_y,\n",
    "        propensity=pd.DataFrame(clf.predict_proba(valid_X)).iloc[:,1],\n",
    "        cutoff=0.5))\n",
    "\n",
    "adabooster_reg_proba = clf.predict_proba(valid_X)\n",
    "\n",
    "df_valid=pd.DataFrame()\n",
    "df_valid['class']=valid_y\n",
    "df_valid['prob_0']=adabooster_reg_proba[:,0]\n",
    "df_valid['prob_1']=adabooster_reg_proba[:,1]\n",
    "df_valid['y_pred']=[1 if p > 0.5 else 0 for p in df_valid.prob_1]\n",
    "df_valid\n",
    "print('-'*50)\n",
    "print(\"accuracy\")\n",
    "print(accuracy_score(df_valid['class'],df_valid['y_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaggingClassifier with DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
      "                                                        class_weight=None,\n",
      "                                                        criterion='gini',\n",
      "                                                        max_depth=6,\n",
      "                                                        max_features=10,\n",
      "                                                        max_leaf_nodes=None,\n",
      "                                                        min_impurity_decrease=0.0,\n",
      "                                                        min_impurity_split=None,\n",
      "                                                        min_samples_leaf=5,\n",
      "                                                        min_samples_split=2,\n",
      "                                                        min_weight_fraction_leaf=0.0,\n",
      "                                                        presort='deprecated',\n",
      "                                                        random_state=1,\n",
      "                                                        splitter='best'),\n",
      "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "                  max_samples=1.0, n_estimators=100, n_jobs=None,\n",
      "                  oob_score=False, random_state=1, verbose=0, warm_start=False)\n",
      "('true negative 318', 'false positive 37', 'false negative 100', 'true positive 334', 'true positive rate 0.7695852534562212', 'sensitivity 0.7695852534562212', 'recall 0.7695852534562212', 'hit rate 0.7695852534562212', 'precision 0.9002695417789758', 'positive predictive value 0.9002695417789758')\n",
      "--------------------------------------------------\n",
      "accuracy\n",
      "0.826362484157161\n"
     ]
    }
   ],
   "source": [
    "clf = BaggingClassifier(DecisionTreeClassifier(max_depth=6,min_samples_split=2,\n",
    "                                               min_samples_leaf=5,max_features=10,random_state=1), \n",
    "                                               n_estimators=100, random_state=1)\n",
    "\n",
    "param_grid = {}\n",
    "gridSearch = GridSearchCV(clf, \n",
    "                          param_grid, cv=5, n_jobs=-1)\n",
    "gridSearch.fit(train_X, train_y)\n",
    "\n",
    "clf = gridSearch.best_estimator_\n",
    "\n",
    "print(clf)\n",
    "\n",
    "print(eval_tp(actual=valid_y,\n",
    "        propensity=pd.DataFrame(clf.predict_proba(valid_X)).iloc[:,1],\n",
    "        cutoff=0.5))\n",
    "\n",
    "bagging_reg_proba = clf.predict_proba(valid_X)\n",
    "\n",
    "df_valid=pd.DataFrame()\n",
    "df_valid['class']=valid_y\n",
    "df_valid['prob_0']=bagging_reg_proba[:,0]\n",
    "df_valid['prob_1']=bagging_reg_proba[:,1]\n",
    "df_valid['y_pred']=[1 if p > 0.5 else 0 for p in df_valid.prob_1]\n",
    "df_valid\n",
    "print('-'*50)\n",
    "print(\"accuracy\")\n",
    "print(accuracy_score(df_valid['class'],df_valid['y_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
      "                       warm_start=False)\n",
      "('true negative 312', 'false positive 43', 'false negative 80', 'true positive 354', 'true positive rate 0.815668202764977', 'sensitivity 0.815668202764977', 'recall 0.815668202764977', 'hit rate 0.815668202764977', 'precision 0.8916876574307305', 'positive predictive value 0.8916876574307305')\n",
      "--------------------------------------------------\n",
      "accuracy\n",
      "0.844106463878327\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "\n",
    "param_grid = {}\n",
    "gridSearch = GridSearchCV(clf, \n",
    "                          param_grid, cv=5, n_jobs=-1,)\n",
    "gridSearch.fit(train_X, train_y)\n",
    "\n",
    "clf = gridSearch.best_estimator_\n",
    "\n",
    "print(clf)\n",
    "\n",
    "print(eval_tp(actual=valid_y,\n",
    "        propensity=pd.DataFrame(clf.predict_proba(valid_X)).iloc[:,1],\n",
    "        cutoff=0.5))\n",
    "\n",
    "forest_reg_proba = clf.predict_proba(valid_X)\n",
    "\n",
    "df_valid=pd.DataFrame()\n",
    "df_valid['class']=valid_y\n",
    "df_valid['prob_0']=forest_reg_proba[:,0]\n",
    "df_valid['prob_1']=forest_reg_proba[:,1]\n",
    "df_valid['y_pred']=[1 if p > 0.5 else 0 for p in df_valid.prob_1]\n",
    "df_valid\n",
    "print('-'*50)\n",
    "print(\"accuracy\")\n",
    "print(accuracy_score(df_valid['class'],df_valid['y_pred']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaggingClassifier accuracy is less( 0.826362484157161 ) when compared with RandomForestClassifier which is 0.844106463878327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "('true negative 333', 'false positive 22', 'false negative 78', 'true positive 356', 'true positive rate 0.8202764976958525', 'sensitivity 0.8202764976958525', 'recall 0.8202764976958525', 'hit rate 0.8202764976958525', 'precision 0.9417989417989417', 'positive predictive value 0.9417989417989417')\n",
      "--------------------------------------------------\n",
      "accuracy\n",
      "0.8732572877059569\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "param_grid = {}\n",
    "\n",
    "gridSearch = GridSearchCV(clf, \n",
    "                          param_grid, cv=5, n_jobs=-1)\n",
    "gridSearch.fit(train_X, train_y)\n",
    "\n",
    "clf = gridSearch.best_estimator_\n",
    "\n",
    "print(clf)\n",
    "\n",
    "print(eval_tp(actual=valid_y,\n",
    "        propensity=pd.DataFrame(clf.predict_proba(valid_X)).iloc[:,1],\n",
    "        cutoff=0.5))\n",
    "\n",
    "boosting_reg_proba = clf.predict_proba(valid_X)\n",
    "\n",
    "df_valid=pd.DataFrame()\n",
    "df_valid['class']=valid_y\n",
    "df_valid['prob_0']=boosting_reg_proba[:,0]\n",
    "df_valid['prob_1']=boosting_reg_proba[:,1]\n",
    "df_valid['y_pred']=[1 if p > 0.5 else 0 for p in df_valid.prob_1]\n",
    "df_valid\n",
    "print('-'*50)\n",
    "print(\"accuracy\")\n",
    "print(accuracy_score(df_valid['class'],df_valid['y_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoostingClassifier has higher accuracy 0.8732572877059569 when compared with BaggingClassifier accuracy and RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
